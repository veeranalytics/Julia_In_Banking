{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages \n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Confusion Matrix\n",
    "function confusion_matrix(y_true::Array{Int64,1},y_pred::Array{Int64,1})\n",
    "    # Generate confusion matrix\n",
    "    classes = sort(unique([unique(y_true),unique(y_pred)]))\n",
    "    cm = zeros(Int64,length(classes),length(classes))\n",
    "\n",
    "    for i in 1:length(y_test)\n",
    "        # translate label to index\n",
    "        true_class = findfirst(classes,y_test[i])\n",
    "        pred_class = findfirst(classes,y_pred[i])\n",
    "        # pred class is the row, true class is the column\n",
    "        cm[pred_class,true_class] += 1\n",
    "    end\n",
    "    cm\n",
    "end\n",
    "\n",
    "# Example: confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Data into Train and Test datasets\n",
    "function partitionTrainTest(data, at = 0.7)\n",
    "    n = nrow(data)\n",
    "    idx = shuffle(1:n)\n",
    "    train_idx = view(idx, 1:floor(Int, at*n))\n",
    "    test_idx = view(idx, (floor(Int, at*n)+1):n)\n",
    "    data[train_idx,:], data[test_idx,:]\n",
    "end\n",
    "\n",
    "# Example: train,test = partitionTrainTest(data, 0.7) # 70% train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a standard scaler function\n",
    "type StandardScalar\n",
    "    mean::Vector{Float64}\n",
    "    std::Vector{Float64}\n",
    "end\n",
    "\n",
    "# Helper function to initialize an empty scalar\n",
    "function StandardScalar()\n",
    "    StandardScalar(Array(Real,0),Array(Real,0))\n",
    "end\n",
    "\n",
    "# Compute mean and standard deviation of each column\n",
    "function fit_std_scalar!(std_scalar::StandardScalar,X::Matrix{Real})\n",
    "    n_rows, n_cols = size(X_test)\n",
    "    std_scalar.std = zeros(n_cols)\n",
    "    std_scalar.mean = zeros(n_cols)\n",
    "    # for loops are fast again!\n",
    "    for i = 1:n_cols\n",
    "        std_scalar.mean[i] = mean(X[:,i])\n",
    "        std_scalar.std[i] = std(X[:,i])\n",
    "    end\n",
    "end\n",
    "\n",
    "function transform(std_scalar::StandardScalar,X::Matrix{Real})\n",
    "    (X .- std_scalar.mean') ./ std_scalar.std' # broadcasting fu\n",
    "end\n",
    "\n",
    "# fit and transform in one function\n",
    "function fit_transform!(std_scalar::StandardScalar,X::Matrix{Real})\n",
    "    fit_std_scalar!(std_scalar,X)\n",
    "    transform(std_scalar,X)\n",
    "end\n",
    "\n",
    "# Examples:\n",
    "# Perform Standard Scaling for all X variables\n",
    "# std_scalar = StandardScalar()\n",
    "# X_train = fit_transform!(std_scalar,X_train)\n",
    "# X_test = transform(std_scalar,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "function predict(data, model_params) \n",
    "\t1.0 ./ (1.0 + exp(-data * model_params[:omega]  - model_params[:beta]))\n",
    "end \n",
    "\n",
    "function _mult(a::Array{Float64,1},b::Array{Float64,2})\n",
    "    result = zeros(length(a))\n",
    "    both_non_zero_indicator = ((a .!= 0) &amp; (b .!= 0))\n",
    "    result[both_non_zero_indicator[:]] = a[both_non_zero_indicator[:]] .* b[both_non_zero_indicator]\n",
    "    return result\n",
    "end\n",
    "\n",
    "\n",
    "function goal_function(omega::Array{Float64,2}, beta::Float64, data::Array{Float64,2}, labels::Array{Float64,1})\n",
    "    f_partial = 1.0 ./ (1.0 + exp(-data * omega  - beta))\n",
    "    result = -sum(_mult(labels, log(f_partial)) + _mult((1.0 - labels), log(1.0 - f_partial)))\n",
    "    return result\n",
    "end\n",
    "\n",
    "function convergence(omega::Array{Float64,2}, beta::Float64, data::Array{Float64,2}, labels::Array{Float64,1},  prevJ::Float64, epsilon::Float64)\n",
    "     currJ = goal_function(omega, beta, data, labels)\n",
    "     return abs(prevJ - currJ) &lt; epsilon\n",
    "end\n",
    "\n",
    "function update_params(omega::Array{Float64,2}, beta::Float64,data::Array{Float64,2}, labels::Array{Float64,1}, alpha::Float64)\n",
    "    partial_derivative = (1.0 ./ (1.0 + exp(-data * omega  - beta)))  - labels\n",
    "    omega = omega - alpha *  (partial_derivative' * data)'\n",
    "    beta = beta  - alpha * sum(partial_derivative)\n",
    "    return omega,beta\n",
    "end\n",
    "\n",
    "function logistic_regression(data::Array{Float64,2}, labels::Array{Float64,1}, params::Dict{Symbol,Float64})\n",
    "\n",
    "    omega = zeros(Float64, size(data,2),1)\n",
    "    beta = 0.0\n",
    "    J = Inf\n",
    "    current_iter = 0\n",
    "    alpha_step, epsilon, max_iter = params[:alpha], params[:eps], params[:max_iter]\n",
    "\n",
    "    while !convergence(omega, beta, data, labels, J, epsilon) &amp;&amp; current_iter &lt; max_iter\n",
    "         J = goal_function(omega, beta, data, labels)\n",
    "         omega, beta = update_params(omega, beta, data, labels, alpha_step)\n",
    "         current_iter += 1\n",
    "    end\n",
    "    model_params = Dict();\n",
    "    model_params[:omega] = omega; model_params[:beta] = beta;\t\n",
    "    return model_params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNetwork = function(params)\n",
    "    instance = x -> f(x,params) #  Instance of a neural network is the set of function parameters [params] + some constant behaviour [f].\n",
    "    return instance \n",
    "end\n",
    "\n",
    "function appendColumnOfOnes(a::Array{Float64,2})\n",
    "  vcat(a,ones(1,size(a ,2)))\n",
    "end\n",
    "\n",
    "function sigmoidNeuronTransformFunction(params, input)\n",
    "  return 1.0 ./ (1.0 .+ exp(-params * appendColumnOfOnes(input)))\n",
    "end\n",
    "\n",
    "abstract Layer\n",
    "\n",
    "type FullyConnectedComputingLayer <: Layer\n",
    "  inputSize::Int64\n",
    "  numberOfNeurons::Int64\n",
    "  parameters::Array{Float64,2}\n",
    "  transform::Function # we don't define the function directly to get flexibility later \n",
    "\n",
    "  function FullyConnectedComputingLayer(inputSize, numberOfNeurons, transform::Function)\n",
    "    parameters = randn(numberOfNeurons, inputSize + 1) # adding one param column for bias\n",
    "    return new(inputSize, numberOfNeurons, parameters, transform)\n",
    "  end\n",
    "end\n",
    "\n",
    "type NetworkArchitecture\n",
    "  layers::Array{Layer}\n",
    "  function NetworkArchitecture(firstLayer::Layer)\n",
    "    return new([firstLayer])\n",
    "  end\n",
    "end\n",
    "\n",
    "function addFullyConnectedSigmoidLayer(architecture::NetworkArchitecture, numberOfNeurons::Int64)\n",
    " lastNetworkLayer = architecture.layers[end]\n",
    " inputSize = lastNetworkLayer.numberOfNeurons\n",
    " sigmoidLayer = FullyConnectedComputingLayer(inputSize, numberOfNeurons, sigmoidNeuronTransformFunction)\n",
    " push!(architecture.layers, sigmoidLayer)\n",
    "end\n",
    "\n",
    "sigmoidLayer = FullyConnectedComputingLayer(15, 32, sigmoidNeuronTransformFunction) \n",
    "architecture = NetworkArchitecture(sigmoidLayer)\n",
    "size(architecture.layers[end].parameters)\n",
    "# (15,32)\n",
    "\n",
    "addFullyConnectedSigmoidLayer(architecture, 16)\n",
    "size(architecture.layers[end].parameters)\n",
    "# (16,32)\n",
    "\n",
    "function infer(architecture::NetworkArchitecture, input)\n",
    "  currentResult = input\n",
    "  for i in 1:length(architecture.layers)\n",
    "     layer = architecture.layers[i]\n",
    "     currentResult = layer.transform(layer.parameters, currentResult)\n",
    "  end\n",
    "  return currentResult\n",
    "end\n",
    "\n",
    "firstLayer = FullyConnectedComputingLayer(14, 2031, sigmoidNeuronTransformFunction)\n",
    "architecture = NetworkArchitecture(firstLayer)\n",
    "addFullyConnectedSigmoidLayer(architecture, 16)\n",
    "addFullyConnectedSigmoidLayer(architecture, 8)\n",
    "addFullyConnectedSigmoidLayer(architecture, 1)\n",
    "infer(architecture , features) # we try a single vector now\n",
    "# 5x1 Array{Float64,2}:\n",
    "# 0.217054\n",
    "# 0.681068\n",
    "# 0.149886\n",
    "# 0.772127\n",
    "# 0.458145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sigmoid(z)\n",
    "  # sigmoid is a basic sigmoid function returning values from 0-1\n",
    "  1. / ( 1. + 1e.^(-z) )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sigmoidGradient(z)\n",
    "  sigmoid(z) .* ( 1 - sigmoid(z) )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_theta(input_unit_count, output_class_count, hidden_unit_length_list)\n",
    "  #\n",
    "  #initialize_theta creates architecture of neural network\n",
    "  #    \n",
    "  #Parameters:\n",
    "  #  hidden_unit_length_list - Array of hidden layer units\n",
    "  #  input_unit_count - integer, number of input units (features)\n",
    "  #  output_class_count - integer, number of output classes\n",
    "  #\n",
    "  #Returns:\n",
    "  #  \n",
    "  #Array of theta arrays randomly initialized to from -.5 to .5\n",
    "  #\n",
    "  \n",
    "  if length( hidden_unit_length_list ) == 0\n",
    "    hidden_unit_length_list = [2]\n",
    "  end\n",
    "  \n",
    "  # unit_count_list = [input_unit_count]\n",
    "  # unit_count_list = [unit_count_list, hidden_unit_length_list]\n",
    "  # unit_count_list = [unit_count_list, output_class_count]\n",
    "  unit_count_list = [input_unit_count, output_class_count]\n",
    "  layers = length(unit_count_list)\n",
    "  \n",
    "  Theta_L = [rand(unit_count_list[i], unit_count_list[i-1]+1) - 0.5 for i = 2:layers]\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function print_theta(Theta_L)\n",
    "  # print_theta() is a helper function that prints Theta_L and architecture info\n",
    "  # It does not actually \"do\" anything except print to stdout\n",
    "\n",
    "  T = length(Theta_L)\n",
    "\n",
    "  println()\n",
    "  println(\"NN ARCHITECTURE\")\n",
    "  println(\"$(T+1) Layers ($(T-1) Hidden)\")\n",
    "  println(\"$T Thetas\")\n",
    "  println(\"$(size(Theta_L[1],2)-1) Input Features\")\n",
    "  println(\"$(size(Theta_L[end], 1)) Output Classes\")\n",
    "  println()\n",
    "    \n",
    "  println(\"Units per layer (excl. bias unit)\")\n",
    "  for t = 1:T\n",
    "    if t == 1\n",
    "      println(\" - Input: $(size(Theta_L[t],2)-1) Units\")\n",
    "    end\n",
    "    if t < T\n",
    "      println(\" - Hidden $t: $(size(Theta_L[t],1)) Units\")\n",
    "    else\n",
    "      println(\" - Output: $(size(Theta_L[t],1)) Units\")\n",
    "    end\n",
    "  end\n",
    "  println()\n",
    "\n",
    "  println(\"Theta Shapes\")\n",
    "  for l = 1:T\n",
    "    println(\"Theta $l: $(size(Theta_L[l]))\")\n",
    "  end\n",
    "  println()\n",
    "  \n",
    "  println(\"Theta Values\")\n",
    "  for t= 1:T\n",
    "    println(\"Theta $t:\" )\n",
    "    println(Theta_L[t])\n",
    "  end\n",
    "  println()\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nn_cost(Y, Y_pred)\n",
    "  #\n",
    "  # nn_cost implements cost function for array inputs Y and Y_pred\n",
    "  #  \n",
    "  # y is array of n_observations by n_classes\n",
    "  # Y_pred is array with same dimensions as Y of predicted y values\n",
    "  #\n",
    "  if size(Y) != size(Y_pred)\n",
    "    if size(Y,1) != size(Y_pred,1)\n",
    "      error(\"Wrong number of predictions\", \"$(size(Y,1)) Actual Values. $(size(Y_pred,1)) Predicted Values. \")\n",
    "    else\n",
    "      error(\"Wrong number of prediction classes\", \"$(size(Y,2)) Actual Classes. $(size(Y_pred,2)) Predicted Classes. \")\n",
    "    end\n",
    "  end\n",
    "    \n",
    "  n_observations = size(Y,1)\n",
    "  \n",
    "  # Cost Function\n",
    "  J = (-1.0 / n_observations ) * sum((Y .* log(Y_pred)) + ((1-Y) .* log(1-Y_pred)))\n",
    "  \n",
    "  J\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nn_predict(X, Theta_L)\n",
    "  #\n",
    "  # nn_predict calculates activations for all layers given X and thetas in Theta_L\n",
    "  # return all inputs and activations for all layers for use in backprop\n",
    "  #\n",
    "  # Parameters\n",
    "  #  X is matrix of input features dimensions n_observations by n_features\n",
    "  #  Theta_L is a 3D array where first element corresponds to the layer number, second is unit at layer+1, third is unit in layer\n",
    "  #\n",
    "  # Returns\n",
    "  #  a_N - 1D Array of activation 2D arrays for each layer: Input (1), Hidden (2 to T), and Output (T+1)\n",
    "  #  a_Z - 1D Array of input 2D arrays to compute activations for all non-bias units\n",
    "  #  a_N[end] - 2D Array of predicted Y values with dimensions n_observations by n_classes\n",
    "  #\n",
    "\n",
    "  a_N = Any[]\n",
    "  z_N = Any[]\n",
    "\n",
    "  m = size(X,1)\n",
    "  T = length(Theta_L)\n",
    "    \n",
    "  # Input Layer inputs\n",
    "  push!(a_N, X) # List of activations including bias unit for non-output layers\n",
    "  push!(z_N, zeros(1,1)) # add filler Z layer to align keys/indexes for a, z, and Theta\n",
    "\n",
    "  # Loop through each Theta_List theta\n",
    "  # t is index of Theta for calculating layer t+1 from layer t\n",
    "  for t=1:T\n",
    "    # Reshape 1D Array into 2D Array\n",
    "    if ndims(a_N[t]) == 1\n",
    "      a_N[t] = reshape(a_N[t], 1, size(a_N[t],1))\n",
    "    end\n",
    "\n",
    "    # Add bias unit\n",
    "    a_N[t] = [ones(size(a_N[t],1), 1)  a_N[t]]\n",
    "      \n",
    "    # Calculate and Append new z and a arrays to z_N and a_N lists\n",
    "    push!(z_N, a_N[t] * Theta_L[t]')\n",
    "    push!(a_N, sigmoid(z_N[t+1]))\n",
    "  end\n",
    "  \n",
    "  z_N, a_N, a_N[end]\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function back_prop(X_train, Y_train, Theta_L, lmda)\n",
    "  #\n",
    "  # Parameters\n",
    "  #  X_train - Array of feature inputs with dimensions n_observations by n_features\n",
    "  #  Y_train - Array of class outputs with dimensions n_observations by n_classes\n",
    "  #  Theta_L is a 1D array of Theta values where 1D element is the layer number, the 2D elements are unit in layer+1 and unit in layer\n",
    "  #  lmda - Float64 - lambda term for regularization\n",
    "  #  \n",
    "  # Returns\n",
    "  #  Y_pred as array of predicted Y values from nn_predict()\n",
    "  #  Theta_Gradient_L as 1D array of 2D Theta Gradient arrays\n",
    "  #\n",
    "\n",
    "  n_observations = size(X_train,1)\n",
    "  \n",
    "  T = length(Theta_L)\n",
    "\n",
    "  # Create Modified copy of the Theta_L for Regularization with Coefficient for bias unit set to 0 so that bias unit is not regularized\n",
    "  # Create variable to accumulate error caused by each Theta_L term in layer a_N[n+1]\n",
    "  Theta_Gradient_L = Any[]\n",
    "  regTheta = Any[]\n",
    "  for i=1:T\n",
    "    push!(regTheta, [zeros(size(Theta_L[i],1),1) Theta_L[i][:, :]])\n",
    "    push!(Theta_Gradient_L, zeros(size(Theta_L[i])))\n",
    "  end\n",
    "\n",
    "\n",
    "  # Forward Pass\n",
    "  z_N, a_N, Y_pred = nn_predict(X_train, Theta_L)\n",
    "\n",
    "  # Backprop Error Accumulator\n",
    "  delta_N = Any[]\n",
    "  for t=1:T\n",
    "    push!(delta_N, Any[])\n",
    "  end\n",
    "    \n",
    "  # Error for Output layer is predicted value - Y training value\n",
    "  delta = Y_pred - Y_train\n",
    "  if ndims(delta) == 1\n",
    "    delta = reshape(delta, 1, length(delta) )\n",
    "  end\n",
    "\n",
    "  # Loop backwards through Thetas to apply Error to prior Layer (except input layer)\n",
    "  # Finish at T-2 because start at 0, output layer is done outside, the loop and input has no error\n",
    "\n",
    "  # Output Error\n",
    "  delta_N[T] = delta\n",
    "\n",
    "  # Hidden Layers Error    \n",
    "  for t=0:T-2\n",
    "    delta = (delta * Theta_L[T-t][:, :]) .* sigmoidGradient(z_N[T-t])\n",
    "    delta_N[T-t-1] = delta\n",
    "  end\n",
    "    \n",
    "  # Calculate error gradients (no error in input layer)\n",
    "  # t is the Theta from layer t to layer t+1\n",
    "  for t=1:T\n",
    "    Theta_Gradient_L[t] = Theta_Gradient_L[t] + delta_N[t]' * a_N[t] #'\n",
    "  end\n",
    "\n",
    "  # Average Error + regularization penalty  \n",
    "  for t=1:T\n",
    "    Theta_Gradient_L[t] = Theta_Gradient_L[t] * (1.0/n_observations) + (lmda * regTheta[t])\n",
    "  end\n",
    "  \n",
    "  Y_pred, Theta_Gradient_L\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fit(X_train, Y_train, Theta_L, lmda, epochs)\n",
    "  #\n",
    "  #fit() calls the training back_prop function for the given number of cycles\n",
    "  #tracks error and error improvement rates\n",
    "  #  \n",
    "  #Parameters:\n",
    "  #  X_train - Array of training data with dimension n_observations by n_features\n",
    "  #  Y_train - Array of training classes with dimension n_observations by n_classes\n",
    "  #  Theta_L - 1D array of theta 2d arrays where each theta has dimensions n_units[layer+1] by n_units[layer]+1\n",
    "  #  epochs -  integer of number of times to update Theta_L\n",
    "  #  \n",
    "  #Returns\n",
    "  #  Theta_L - 1D array of Theta arrays\n",
    "  #  J_List - Array (length = epochs) of result of cost function for each iteration\n",
    "\n",
    "  J_list = zeros(epochs)\n",
    "\n",
    "  for i=1:epochs\n",
    "    # Back prop to get Y_pred and Theta gradient\n",
    "    Y_pred, Theta_grad = back_prop(X_train, Y_train, Theta_L, lmda)\n",
    "    # Record cost\n",
    "    J_list[i] = nn_cost(Y_train, Y_pred)\n",
    "    # Update Theta using Learning Rate * Theta Gradient\n",
    "    for t=1:length(Theta_L)\n",
    "      # Start with a large learning rate; need to update this to be more intelligent than simply looking at iteration count\n",
    "      # Need to update to change learning rate based on progress of cost function\n",
    "      if i < 100\n",
    "        learning_rate = 5.0        \n",
    "      else\n",
    "        learning_rate = 1.0\n",
    "      end\n",
    "      Theta_L[t] = Theta_L[t] - ( learning_rate * Theta_grad[t] )\n",
    "    end\n",
    "    #println(\"Cost $i: $(J_list[i])\")\n",
    "  end\n",
    "\n",
    "  Theta_L, J_list\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "  Theta_L, J_list = fit(features, labels, Theta_L, lmda, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer architecture\n",
    "  hidden_unit_length_list = [2]\n",
    "  hidden_layer_architecture = hidden_unit_length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization Term\n",
    "  lmda = 1e-5\n",
    "  epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Theta based on selected architecture\n",
    "  Theta_L = initialize_theta(size(features,2), size(labels,2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "  Theta_L, J_list = fit(features, labels, Theta_L, lmda, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Architecture\n",
    "  print_theta(Theta_L)\n",
    "  \n",
    "# Print Cost\n",
    "  println(\"Cost Function Applied to Training Data: $(J_list[end])\")\n",
    "\n",
    "# Predict\n",
    "  X_new = [1 0;0 1;1 1;0 0]\n",
    "  println( \"Given X: $X_new\")\n",
    "  z_N, a_N, Y_pred = nn_predict(X_new, Theta_L)\n",
    "  println( \"Predicted Y: $(round(Y_pred,3))\")\n",
    "\n",
    "  Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function XOR_test(hidden_unit_length_list, epochs)\n",
    "  #\n",
    "  #XOR_test is a simple test of the nn printing the predicted value to std out\n",
    "  #Trains on a sample XOR data set\n",
    "  #Predicts a single value\n",
    "  #Accepts an option parameter to set architecture of hidden layers\n",
    "  #\n",
    "  \n",
    "  \n",
    "  println( \"Training Data: X & Y\")\n",
    "\n",
    "  # Training Data\n",
    "  X_train = [1 1; 1 0; 0 1; 0 0]\t# Training Input Data\n",
    "  Y_train = [0 1; 1 0; 1 0; 0 1] \t\t\t# Training Classes\n",
    "  println( X_train )\n",
    "  println( Y_train )\n",
    "  \n",
    "  # Hidden layer architecture\n",
    "  hidden_layer_architecture = hidden_unit_length_list\n",
    "\n",
    "  # Regularization Term\n",
    "  lmda = 1e-5\n",
    "\n",
    "  # Initialize Theta based on selected architecture\n",
    "  Theta_L = initialize_theta(size(X_train,2), size(Y_train,2), hidden_layer_architecture)\n",
    "\n",
    "  \n",
    "  # Fit\n",
    "  Theta_L, J_list = fit(X_train, Y_train, Theta_L, lmda, epochs)\n",
    "  \n",
    "  # Print Architecture\n",
    "  print_theta(Theta_L)\n",
    "  \n",
    "  # Print Cost\n",
    "  println(\"Cost Function Applied to Training Data: $(J_list[end])\")\n",
    "\n",
    "  # Predict\n",
    "  X_new = [1 0;0 1;1 1;0 0]\n",
    "  println( \"Given X: $X_new\")\n",
    "  z_N, a_N, Y_pred = nn_predict(X_new, Theta_L)\n",
    "  println( \"Predicted Y: $(round(Y_pred,3))\")\n",
    "\n",
    "  Y_pred\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "Y_pred = XOR_test([2], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
